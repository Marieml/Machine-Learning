{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKAyneFM9euY"
      },
      "source": [
        "Name: Maria Miliou \\\n",
        "Course: Machine Learning\n",
        "# Assignment 3 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save data to your drive and mount"
      ],
      "metadata": {
        "id": "5oWFJfgqkdE5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3plDg_p9dpW",
        "outputId": "1cfa0a91-ed52-4639-f136-59f282063d93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13z2wGwc-G4G"
      },
      "source": [
        "## **Feedforward Neural Network**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29kPspHM-QMK"
      },
      "source": [
        "###**Step 1**: Loading data (mfccs)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKuGilKxTCqW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader #. It represents a Python iterable over a dataset, with support for\n",
        "\n",
        "\n",
        "# Dictionary for mapping between name of classes and integers 0-3\n",
        "map_labels = {\"blues\": 0, \"classical\": 1, \"hiphop\": 2, \"rock_metal_hardrock\": 3}\n",
        "\n",
        "# Create custom dataset\n",
        "\n",
        "class MfccDataset(Dataset):\n",
        "    def __init__(self, labels_file, X_file, transform = None):\n",
        "        labels = np.load(labels_file)\n",
        "        self.labels = [map_labels[x] for x in labels]  # Transform labels from string to int\n",
        "        self.X = np.load(X_file)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      sample = {'X': self.X[idx], 'label' : self.labels[idx]}\n",
        "      if self.transform:\n",
        "        sample = self.transform(sample)\n",
        "        \n",
        "      return sample\n",
        "\n",
        "# Define transform for transforming data from numpy arrays to tensor \n",
        "class ToTensor(object):\n",
        "    def __call__(self, sample):\n",
        "      X, label = sample['X'], sample['label']\n",
        "      return { 'X': torch.from_numpy(X), 'label' : label}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mosk9I92n0We"
      },
      "outputs": [],
      "source": [
        "# Train, validation and test datasets\n",
        "\n",
        "train_dataset = MfccDataset(\n",
        "    labels_file=\"/content/drive/MyDrive/data/music_genre_data_di/train/mfccs/labels.npy\",\n",
        "    X_file= \"/content/drive/MyDrive/data/music_genre_data_di/train/mfccs/X.npy\",\n",
        "    transform = ToTensor())\n",
        "\n",
        "val_dataset = MfccDataset(\n",
        "    labels_file=\"/content/drive/MyDrive/data/music_genre_data_di/val/mfccs/labels.npy\",\n",
        "    X_file= \"/content/drive/MyDrive/data/music_genre_data_di/val/mfccs/X.npy\",\n",
        "    transform = ToTensor())\n",
        "\n",
        "test_dataset = MfccDataset(\n",
        "    labels_file=\"/content/drive/MyDrive/data/music_genre_data_di/test/mfccs/labels.npy\",\n",
        "    X_file= \"/content/drive/MyDrive/data/music_genre_data_di/test/mfccs/X.npy\",\n",
        "    transform = ToTensor())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xde4E8RflFfr"
      },
      "outputs": [],
      "source": [
        "# Loading data\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrtGXL9lQbuR"
      },
      "source": [
        "###**Step 2**: Definition of Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqTDMKUWEw2S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(26, 128),\n",
        "            #nn.ReLU(),\n",
        "            nn.Linear(128, 32),\n",
        "            #nn.ReLU(),\n",
        "            nn.Linear(32, 4),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJv6zPDRfpPh"
      },
      "source": [
        "###**Step 3**: Definition of training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMnciURsN1VV"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, Nepochs):\n",
        "  size = len(dataloader.dataset)\n",
        "  for epoch in range(Nepochs):\n",
        "      print(f\"-------------------------------\\nEpoch {epoch+1}\\n-------------------------------\")\n",
        "      for batch, sample in enumerate(dataloader):\n",
        "        X = sample['X']\n",
        "        y = sample['label']\n",
        "\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X.float())\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        #per 50 batches report the value of the loss function on the training set\n",
        "        if batch % 50 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD3BwTscmKcx"
      },
      "source": [
        "###**Step 4**: Definition of Testing function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwpyShzvul-m"
      },
      "outputs": [],
      "source": [
        "\n",
        "def test_fn(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    test_loss, correct, f1 = 0, 0, 0\n",
        "\n",
        "    # the no_grad command tells PyTorch that we dont want to accumulate gradients for these operations. Disabling gradient calculation is useful for inference with no backpropagation\n",
        "    with torch.no_grad():\n",
        "        for sample in dataloader:\n",
        "\n",
        "            X = sample['X']\n",
        "            y = sample['label']\n",
        "\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            \n",
        "            pred = model(X.float())\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "            f1 += f1_score(y.to('cpu'), pred.to('cpu').argmax(1), average = 'macro')\n",
        "            c_matrix = confusion_matrix(y.to('cpu'), pred.to('cpu').argmax(1))\n",
        "\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    f1 /= size\n",
        "    \n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}, Avg f1: {f1:>8f} \\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXuzS446n9iv"
      },
      "source": [
        "### **Step 5**: Training NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "461zwRofoEh0",
        "outputId": "703bac81-c901-4c5c-b9db-56faa7fb7e34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device cpu\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.460196  [    0/ 3200]\n",
            "loss: 1.382018  [  800/ 3200]\n",
            "loss: 1.377698  [ 1600/ 3200]\n",
            "loss: 1.447004  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.373881  [    0/ 3200]\n",
            "loss: 1.388828  [  800/ 3200]\n",
            "loss: 1.320949  [ 1600/ 3200]\n",
            "loss: 1.410554  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.427715  [    0/ 3200]\n",
            "loss: 1.348772  [  800/ 3200]\n",
            "loss: 1.399906  [ 1600/ 3200]\n",
            "loss: 1.364457  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.339833  [    0/ 3200]\n",
            "loss: 1.434092  [  800/ 3200]\n",
            "loss: 1.331324  [ 1600/ 3200]\n",
            "loss: 1.395113  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.407538  [    0/ 3200]\n",
            "loss: 1.346398  [  800/ 3200]\n",
            "loss: 1.351049  [ 1600/ 3200]\n",
            "loss: 1.400627  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.325899  [    0/ 3200]\n",
            "loss: 1.318393  [  800/ 3200]\n",
            "loss: 1.309055  [ 1600/ 3200]\n",
            "loss: 1.293900  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.275407  [    0/ 3200]\n",
            "loss: 1.287994  [  800/ 3200]\n",
            "loss: 1.272691  [ 1600/ 3200]\n",
            "loss: 1.255929  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.243177  [    0/ 3200]\n",
            "loss: 1.317776  [  800/ 3200]\n",
            "loss: 1.322206  [ 1600/ 3200]\n",
            "loss: 1.283911  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.308297  [    0/ 3200]\n",
            "loss: 1.316599  [  800/ 3200]\n",
            "loss: 1.234191  [ 1600/ 3200]\n",
            "loss: 1.289867  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.243477  [    0/ 3200]\n",
            "loss: 1.259889  [  800/ 3200]\n",
            "loss: 1.229381  [ 1600/ 3200]\n",
            "loss: 1.252915  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 1.221711  [    0/ 3200]\n",
            "loss: 1.219986  [  800/ 3200]\n",
            "loss: 1.268572  [ 1600/ 3200]\n",
            "loss: 1.174283  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 1.119168  [    0/ 3200]\n",
            "loss: 1.253096  [  800/ 3200]\n",
            "loss: 1.136137  [ 1600/ 3200]\n",
            "loss: 1.131755  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 1.239032  [    0/ 3200]\n",
            "loss: 1.169244  [  800/ 3200]\n",
            "loss: 1.045673  [ 1600/ 3200]\n",
            "loss: 1.036363  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 1.188319  [    0/ 3200]\n",
            "loss: 1.086161  [  800/ 3200]\n",
            "loss: 1.088437  [ 1600/ 3200]\n",
            "loss: 0.892883  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 1.004685  [    0/ 3200]\n",
            "loss: 1.037101  [  800/ 3200]\n",
            "loss: 1.034858  [ 1600/ 3200]\n",
            "loss: 0.952899  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.995026  [    0/ 3200]\n",
            "loss: 1.132405  [  800/ 3200]\n",
            "loss: 1.360249  [ 1600/ 3200]\n",
            "loss: 1.135326  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 1.013746  [    0/ 3200]\n",
            "loss: 1.181786  [  800/ 3200]\n",
            "loss: 1.064965  [ 1600/ 3200]\n",
            "loss: 1.072102  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 1.303277  [    0/ 3200]\n",
            "loss: 0.985307  [  800/ 3200]\n",
            "loss: 0.992324  [ 1600/ 3200]\n",
            "loss: 1.170168  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.834799  [    0/ 3200]\n",
            "loss: 1.425217  [  800/ 3200]\n",
            "loss: 1.220314  [ 1600/ 3200]\n",
            "loss: 1.062418  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.965478  [    0/ 3200]\n",
            "loss: 0.982165  [  800/ 3200]\n",
            "loss: 1.170093  [ 1600/ 3200]\n",
            "loss: 1.247235  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 1.054045  [    0/ 3200]\n",
            "loss: 1.052992  [  800/ 3200]\n",
            "loss: 1.104948  [ 1600/ 3200]\n",
            "loss: 1.107565  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.711959  [    0/ 3200]\n",
            "loss: 1.005079  [  800/ 3200]\n",
            "loss: 0.911419  [ 1600/ 3200]\n",
            "loss: 1.188126  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.867208  [    0/ 3200]\n",
            "loss: 1.145695  [  800/ 3200]\n",
            "loss: 0.954670  [ 1600/ 3200]\n",
            "loss: 0.712180  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 1.156422  [    0/ 3200]\n",
            "loss: 1.422927  [  800/ 3200]\n",
            "loss: 1.101771  [ 1600/ 3200]\n",
            "loss: 0.788561  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.765802  [    0/ 3200]\n",
            "loss: 1.096855  [  800/ 3200]\n",
            "loss: 1.174097  [ 1600/ 3200]\n",
            "loss: 1.031267  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 1.016394  [    0/ 3200]\n",
            "loss: 1.427839  [  800/ 3200]\n",
            "loss: 1.190259  [ 1600/ 3200]\n",
            "loss: 1.028340  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 1.076104  [    0/ 3200]\n",
            "loss: 1.339074  [  800/ 3200]\n",
            "loss: 0.632053  [ 1600/ 3200]\n",
            "loss: 1.162709  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.971565  [    0/ 3200]\n",
            "loss: 1.021569  [  800/ 3200]\n",
            "loss: 0.967558  [ 1600/ 3200]\n",
            "loss: 0.750408  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.930148  [    0/ 3200]\n",
            "loss: 0.811740  [  800/ 3200]\n",
            "loss: 0.901818  [ 1600/ 3200]\n",
            "loss: 1.138620  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.810625  [    0/ 3200]\n",
            "loss: 0.902399  [  800/ 3200]\n",
            "loss: 0.897975  [ 1600/ 3200]\n",
            "loss: 0.760989  [ 2400/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 59.7%, Avg loss: 0.060034, Avg f1: 0.018129 \n",
            "\n",
            "CPU time: 3.6981163024902344\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "device = 'cpu'\n",
        "model = NeuralNetwork().to(device)\n",
        "\n",
        "learning_rate = 2e-3\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "num_epochs = 30\n",
        "\n",
        "print(f\"Device {device}\")\n",
        "\n",
        "#Training\n",
        "start_cpu = time.time()\n",
        "train_loop(train_dataloader, model, loss_fn, optimizer, num_epochs)\n",
        "end_cpu = time.time()\n",
        "\n",
        "#Testing\n",
        "test_fn(test_dataloader, model,loss_fn)\n",
        "\n",
        "print(f\"CPU time: {end_cpu - start_cpu}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 6**: Taining NN using GPU\n",
        "\n"
      ],
      "metadata": {
        "id": "loDANvxPQ_LI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "\n",
        "learning_rate = 2e-3\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "num_epochs = 30\n",
        "\n",
        "print(f\"Device {device}\")\n",
        "\n",
        "#Training\n",
        "start_cuda = time.time()\n",
        "train_loop(train_dataloader, model, loss_fn, optimizer, num_epochs)\n",
        "end_cuda = time.time()\n",
        "\n",
        "#Testing\n",
        "test_fn(test_dataloader, model,loss_fn)\n",
        "\n",
        "print(f\"GPU time: {end_cuda - start_cuda}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCMLzwslJAAm",
        "outputId": "90875b8d-e4bb-4b81-efb6-60ab9b473781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device cuda\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.157213  [    0/ 3200]\n",
            "loss: 1.404286  [  800/ 3200]\n",
            "loss: 1.377580  [ 1600/ 3200]\n",
            "loss: 1.365701  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.416197  [    0/ 3200]\n",
            "loss: 1.359858  [  800/ 3200]\n",
            "loss: 1.322867  [ 1600/ 3200]\n",
            "loss: 1.405018  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.297819  [    0/ 3200]\n",
            "loss: 1.314305  [  800/ 3200]\n",
            "loss: 1.303071  [ 1600/ 3200]\n",
            "loss: 1.288286  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.272789  [    0/ 3200]\n",
            "loss: 1.294675  [  800/ 3200]\n",
            "loss: 1.390621  [ 1600/ 3200]\n",
            "loss: 1.278575  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.328136  [    0/ 3200]\n",
            "loss: 1.319416  [  800/ 3200]\n",
            "loss: 1.257396  [ 1600/ 3200]\n",
            "loss: 1.332420  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.300822  [    0/ 3200]\n",
            "loss: 1.322821  [  800/ 3200]\n",
            "loss: 1.176712  [ 1600/ 3200]\n",
            "loss: 1.375907  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.147485  [    0/ 3200]\n",
            "loss: 1.292058  [  800/ 3200]\n",
            "loss: 1.117877  [ 1600/ 3200]\n",
            "loss: 1.170485  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.136211  [    0/ 3200]\n",
            "loss: 1.090538  [  800/ 3200]\n",
            "loss: 1.411965  [ 1600/ 3200]\n",
            "loss: 1.101283  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.112755  [    0/ 3200]\n",
            "loss: 1.256449  [  800/ 3200]\n",
            "loss: 1.204127  [ 1600/ 3200]\n",
            "loss: 1.194697  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.104543  [    0/ 3200]\n",
            "loss: 1.060571  [  800/ 3200]\n",
            "loss: 1.180450  [ 1600/ 3200]\n",
            "loss: 1.154869  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 1.095408  [    0/ 3200]\n",
            "loss: 1.187418  [  800/ 3200]\n",
            "loss: 1.116541  [ 1600/ 3200]\n",
            "loss: 1.012659  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 1.045628  [    0/ 3200]\n",
            "loss: 1.121721  [  800/ 3200]\n",
            "loss: 1.058205  [ 1600/ 3200]\n",
            "loss: 0.945212  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 1.026099  [    0/ 3200]\n",
            "loss: 0.989851  [  800/ 3200]\n",
            "loss: 0.930901  [ 1600/ 3200]\n",
            "loss: 0.900044  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 1.218335  [    0/ 3200]\n",
            "loss: 1.012825  [  800/ 3200]\n",
            "loss: 1.039458  [ 1600/ 3200]\n",
            "loss: 1.329950  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 1.035046  [    0/ 3200]\n",
            "loss: 1.229046  [  800/ 3200]\n",
            "loss: 1.044388  [ 1600/ 3200]\n",
            "loss: 1.057517  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 1.362764  [    0/ 3200]\n",
            "loss: 0.866655  [  800/ 3200]\n",
            "loss: 1.113320  [ 1600/ 3200]\n",
            "loss: 0.918248  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.928982  [    0/ 3200]\n",
            "loss: 1.073122  [  800/ 3200]\n",
            "loss: 0.925491  [ 1600/ 3200]\n",
            "loss: 1.056343  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 1.036858  [    0/ 3200]\n",
            "loss: 1.341508  [  800/ 3200]\n",
            "loss: 0.812960  [ 1600/ 3200]\n",
            "loss: 1.027913  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.842044  [    0/ 3200]\n",
            "loss: 1.093600  [  800/ 3200]\n",
            "loss: 1.205201  [ 1600/ 3200]\n",
            "loss: 0.846099  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 1.118401  [    0/ 3200]\n",
            "loss: 0.953021  [  800/ 3200]\n",
            "loss: 0.934902  [ 1600/ 3200]\n",
            "loss: 0.769235  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.940702  [    0/ 3200]\n",
            "loss: 0.721931  [  800/ 3200]\n",
            "loss: 0.963214  [ 1600/ 3200]\n",
            "loss: 0.946005  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 1.117322  [    0/ 3200]\n",
            "loss: 1.088124  [  800/ 3200]\n",
            "loss: 0.902739  [ 1600/ 3200]\n",
            "loss: 0.675390  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 1.136533  [    0/ 3200]\n",
            "loss: 1.055088  [  800/ 3200]\n",
            "loss: 1.011488  [ 1600/ 3200]\n",
            "loss: 0.935298  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.801948  [    0/ 3200]\n",
            "loss: 0.908265  [  800/ 3200]\n",
            "loss: 1.023710  [ 1600/ 3200]\n",
            "loss: 0.759794  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 1.045429  [    0/ 3200]\n",
            "loss: 1.029935  [  800/ 3200]\n",
            "loss: 0.735529  [ 1600/ 3200]\n",
            "loss: 0.720741  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 1.002555  [    0/ 3200]\n",
            "loss: 1.150593  [  800/ 3200]\n",
            "loss: 0.865752  [ 1600/ 3200]\n",
            "loss: 0.841851  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 1.052117  [    0/ 3200]\n",
            "loss: 1.143382  [  800/ 3200]\n",
            "loss: 0.739508  [ 1600/ 3200]\n",
            "loss: 0.920283  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 1.138821  [    0/ 3200]\n",
            "loss: 0.658702  [  800/ 3200]\n",
            "loss: 0.953071  [ 1600/ 3200]\n",
            "loss: 1.404319  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 1.122272  [    0/ 3200]\n",
            "loss: 0.828072  [  800/ 3200]\n",
            "loss: 0.741446  [ 1600/ 3200]\n",
            "loss: 1.334091  [ 2400/ 3200]\n",
            "-------------------------------\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.731569  [    0/ 3200]\n",
            "loss: 0.915403  [  800/ 3200]\n",
            "loss: 0.926329  [ 1600/ 3200]\n",
            "loss: 0.916188  [ 2400/ 3200]\n",
            "Test Error: \n",
            " Accuracy: 62.8%, Avg loss: 0.058388, Avg f1: 0.018785 \n",
            "\n",
            "GPU time: 6.417418956756592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning's execution time in both GPU and CPU"
      ],
      "metadata": {
        "id": "fdLrGt5Ude8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"CPU time: {end_cpu - start_cpu}\")\n",
        "print(f\"GPU time: {end_cuda - start_cuda}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTlWRjMddeTM",
        "outputId": "74a7f31f-5532-47a2-9d57-7c5a86f5fb69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU time: 3.6981163024902344\n",
            "GPU time: 6.417418956756592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 7**: Choosing \"best\" model\n"
      ],
      "metadata": {
        "id": "Lz4sWF7JfFmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Editing the training procedure to evaluate model's snapshots with validation set."
      ],
      "metadata": {
        "id": "p3MLeniRf0Bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "def train_loop_val(dataloader, model, loss_fn, optimizer, Nepochs):\n",
        "  size = len(dataloader.dataset)\n",
        "\n",
        "  max_val_f1 = 0.0\n",
        "\n",
        "  for epoch in range(Nepochs):\n",
        "\n",
        "      for batch, sample in enumerate(dataloader):\n",
        "        X = sample['X']\n",
        "        y = sample['label']\n",
        "\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X.float())\n",
        "        loss = loss_fn(pred, y) \n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      # Compute f1 score of validation set in this model\n",
        "      val_f1 = 0.0\n",
        "      for batch, sample in enumerate(val_dataloader):\n",
        "          X = sample['X']\n",
        "          y = sample['label']\n",
        "\n",
        "          X = X.to(device)\n",
        "          y = y.to(device)\n",
        "\n",
        "          # Compute prediction\n",
        "          pred = model(X.float())\n",
        "          val_f1 += f1_score(y.cpu(), pred.cpu().argmax(1), average = 'macro')\n",
        "\n",
        "      if val_f1 > max_val_f1:\n",
        "        max_val_f1 = val_f1\n",
        "        print(f\"Found better model in epoch {epoch}! Saving... \")\n",
        "        torch.save(model.state_dict(), 'saved_model.pth')\n",
        "        \n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "juNDjGc5gaa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "\n",
        "train_loop_val(train_dataloader, model, loss_fn, optimizer, num_epochs)\n",
        "\n",
        "best_model = NeuralNetwork().to(device)\n",
        "best_model.load_state_dict(torch.load('saved_model.pth'))\n",
        "best_model.eval()\n",
        "test_fn(test_dataloader, best_model ,loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q44UY-I3lHp6",
        "outputId": "cf54449e-b67b-44e5-82cb-22b207c10a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found better model in epoch 0! Saving... \n",
            "Found better model in epoch 1! Saving... \n",
            "Found better model in epoch 5! Saving... \n",
            "Test Error: \n",
            " Accuracy: 23.8%, Avg loss: 0.092601, Avg f1: 0.010153 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convolutional Neural Network**"
      ],
      "metadata": {
        "id": "AP-L20niIoXd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-Im3DRiJLMP"
      },
      "source": [
        "###**Step 1**: Loading data (spectograms)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "YqrsbUT5QzCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCY-34cZJLMP"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create custom dataset\n",
        "\n",
        "class MelgramDataset(Dataset):\n",
        "    def __init__(self, labels_file, X_file, transform = None):\n",
        "        labels = np.load(labels_file)\n",
        "        self.labels = [map_labels[x] for x in labels]  # Transform labels from string to int\n",
        "        self.X = np.load(X_file)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      sample = {'X': self.X[idx], 'label' : self.labels[idx]}\n",
        "      if self.transform:\n",
        "        sample = self.transform(sample)\n",
        "        \n",
        "      return sample\n",
        "\n",
        "# Define transform for transforming data from numpy arrays to tensor \n",
        "class ToTensor(object):\n",
        "    def __call__(self, sample):\n",
        "      X, label = sample['X'], sample['label']\n",
        "      return { 'X': torch.from_numpy(X), 'label' : label}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJt622jmJLMQ"
      },
      "outputs": [],
      "source": [
        "# Train, validation and test datasets\n",
        "\n",
        "train_dataset = MelgramDataset(\n",
        "    labels_file=\"/content/drive/MyDrive/data/music_genre_data_di/train/melgrams/labels.npy\",\n",
        "    X_file= \"/content/drive/MyDrive/data/music_genre_data_di/train/melgrams/X.npy\",\n",
        "    transform = ToTensor())\n",
        "\n",
        "val_dataset = MelgramDataset(\n",
        "    labels_file=\"/content/drive/MyDrive/data/music_genre_data_di/val/melgrams/labels.npy\",\n",
        "    X_file= \"/content/drive/MyDrive/data/music_genre_data_di/val/melgrams/X.npy\",\n",
        "    transform = ToTensor())\n",
        "\n",
        "test_dataset = MelgramDataset(\n",
        "    labels_file=\"/content/drive/MyDrive/data/music_genre_data_di/test/melgrams/labels.npy\",\n",
        "    X_file= \"/content/drive/MyDrive/data/music_genre_data_di/test/melgrams/X.npy\",\n",
        "    transform = ToTensor())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cxeWT2DJLMQ"
      },
      "outputs": [],
      "source": [
        "# Loading data\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_dq9igkQIz8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Step 2**: Define CNN"
      ],
      "metadata": {
        "id": "1yF61JuuUUZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "\n",
        "class CNNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 16, 5)\n",
        "    self.conv2 = nn.Conv2d(16, 32, 5)\n",
        "    self.conv3 = nn.Conv2d(32, 64, 5)\n",
        "    self.conv4 = nn.Conv2d(64, 128, 5)\n",
        "\n",
        "    self.dense1 = nn.Linear(128*5*112, 1024)\n",
        "    self.dense2 = nn.Linear(1024, 256 )\n",
        "    self.dense3 = nn.Linear(256, 32)\n",
        "    self.dense4 = nn.Linear(32, 4)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = x.unsqueeze(1)\n",
        "      x = x.view(-1, 128*5*112)\n",
        "      x = self.dense1(x)\n",
        "      x = self.dense2(x)\n",
        "      x = self.dense3(x)\n",
        "      x = self.dense4(x)\n",
        "\n",
        "      return x \n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "net= CNNet().to(device)"
      ],
      "metadata": {
        "id": "CDwyV4ZXw94H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Step 3**: Network training\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OtchXDWWbgYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 2e-3\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr = learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "num_epochs = 30\n",
        "\n",
        "print(f\"Device {device}\")\n",
        "\n",
        "train_loop_val(train_dataloader, net, loss_fn, optimizer, num_epochs)\n",
        "\n",
        "best_model = CNNet()\n",
        "best_model.load_state_dict(torch.load('saved_model1.pth'))\n",
        "best_model.eval()\n",
        "test_fn(test_dataloader, best_model ,loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "D4opILQCyBA0",
        "outputId": "fb2e42e1-2a90-4ef3-b9f6-7dd95d510993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device cpu\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-275e596c653e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Device {device}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_loop_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-2816518584a3>\u001b[0m in \u001b[0;36mtrain_loop_val\u001b[0;34m(dataloader, model, loss_fn, optimizer, Nepochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Compute prediction and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-da3649dd93d1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m112\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 71680]' is invalid for input of size 43008"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Network can not be trained because of high dimensionality."
      ],
      "metadata": {
        "id": "fkbXJL5JxxAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Step 4**: Pooling and padding"
      ],
      "metadata": {
        "id": "tDElvzJKxiAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CNNet_pp(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 16, 5, padding=2)\n",
        "    self.conv2 = nn.Conv2d(16, 32, 5, padding=2)\n",
        "    self.conv3 = nn.Conv2d(32, 64, 5, padding=2)\n",
        "    self.conv4 = nn.Conv2d(64, 128, 5, padding=2)\n",
        "\n",
        "    self.dense1 = nn.Linear(1024, 1024)\n",
        "    self.dense2 = nn.Linear(1024, 256 )\n",
        "    self.dense3 = nn.Linear(256, 32)\n",
        "    self.dense4 = nn.Linear(32, 4)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = x.unsqueeze(1)\n",
        "      x = F.max_pool2d(self.conv1(x), kernel_size=2)\n",
        "      x = F.max_pool2d(self.conv2(x), kernel_size=2)\n",
        "      x = F.max_pool2d(self.conv3(x), kernel_size=2)\n",
        "      x = F.max_pool2d(self.conv4(x), kernel_size=2)\n",
        "      x = x.view(-1, 1024)\n",
        "      x = self.dense1(x)\n",
        "      x = self.dense2(x)\n",
        "      x = self.dense3(x)\n",
        "      x = self.dense4(x)\n",
        "\n",
        "      return x \n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "net_pp= CNNet_pp().to(device)\n"
      ],
      "metadata": {
        "id": "-Zw93s4xUoZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "learning_rate = 2e-3\n",
        "optimizer = torch.optim.SGD(net_pp.parameters(), lr = learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "num_epochs = 30\n",
        "\n",
        "print(f\"Device {device}\")\n",
        "\n",
        "train_loop_val(train_dataloader, net_pp, loss_fn, optimizer, num_epochs)\n",
        "\n",
        "best_model = CNNet_pp().to(device)\n",
        "best_model.load_state_dict(torch.load('saved_model.pth'))\n",
        "best_model.eval()\n",
        "test_fn(test_dataloader, best_model ,loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shabbhNq__Mw",
        "outputId": "f295a194-1846-426f-bdc3-0dfc6d65bea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device cpu\n",
            "Found better model in epoch 0! Saving... \n",
            "Found better model in epoch 2! Saving... \n",
            "Found better model in epoch 4! Saving... \n",
            "Found better model in epoch 6! Saving... \n",
            "Found better model in epoch 7! Saving... \n",
            "Found better model in epoch 8! Saving... \n",
            "Found better model in epoch 9! Saving... \n",
            "Found better model in epoch 10! Saving... \n",
            "Found better model in epoch 11! Saving... \n",
            "Found better model in epoch 14! Saving... \n",
            "Test Error: \n",
            " Accuracy: 70.9%, Avg loss: 0.046573, Avg f1: 0.023122 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Step 5**: Optimizers\n",
        "\n",
        "How the use of different optimizers reflex to accuracy anf f1 score\n",
        "\n",
        "|  | Adadelta|Adagrad |SGD | Adam\n",
        "|----|-----|-----|----|-------\n",
        "|Accuracy|73.1%|73.2%|70.9%|70.9%|\n",
        "|f1| 0.025|0.026|0.023|0.022|"
      ],
      "metadata": {
        "id": "GF6RJwK0lIXU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ** 3** \n",
        "## Deep Learning"
      ],
      "metadata": {
        "id": "a1BzLs1ylND6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Step 1**: Reproducibility"
      ],
      "metadata": {
        "id": "msQI87bDzM_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random, os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "seed=0\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)"
      ],
      "metadata": {
        "id": "D9YAk0filJOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNet3(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 16, 5, padding=2)\n",
        "    self.conv2 = nn.Conv2d(16, 32, 5, padding=2)\n",
        "    self.conv3 = nn.Conv2d(32, 64, 5, padding=2)\n",
        "    self.conv4 = nn.Conv2d(64, 128, 5, padding=2)\n",
        "\n",
        "    self.dense1 = nn.Linear(1024, 1024)\n",
        "    self.dense2 = nn.Linear(1024, 256 )\n",
        "    self.dense3 = nn.Linear(256, 32)\n",
        "    self.dense4 = nn.Linear(32, 4)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = x.unsqueeze(1)\n",
        "      x = F.max_pool2d(self.conv1(x), kernel_size=2)\n",
        "      x = F.max_pool2d(self.conv2(x), kernel_size=2)\n",
        "      x = F.max_pool2d(self.conv3(x), kernel_size=2)\n",
        "      x = F.max_pool2d(self.conv4(x), kernel_size=2)\n",
        "      x = x.view(-1, 1024)\n",
        "      x = self.dense1(x)\n",
        "      x = self.dense2(x)\n",
        "      x = self.dense3(x)\n",
        "      x = self.dense4(x)\n",
        "\n",
        "      return x "
      ],
      "metadata": {
        "id": "Lth-4Vk3a1Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "net_seed = CNNet3().to(device)\n",
        "#Training\n",
        "train_loop_val(train_dataloader, net_seed, loss_fn, optimizer, num_epochs)\n",
        "\n",
        "#Take the best model according to validation set\n",
        "best_model = CNNet3().to(device)\n",
        "best_model.load_state_dict(torch.load('saved_model.pth'))\n",
        "best_model.eval()\n",
        "\n",
        "#Testing\n",
        "test_fn(test_dataloader, best_model ,loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS7E_jmLzePi",
        "outputId": "df4c28df-c1d0-4754-a6c4-beaab5966a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found better model in epoch 0! Saving... \n",
            "Found better model in epoch 1! Saving... \n",
            "Found better model in epoch 4! Saving... \n",
            "Test Error: \n",
            " Accuracy: 23.6%, Avg loss: 0.088664, Avg f1: 0.014071 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Step 2**: Activation functions"
      ],
      "metadata": {
        "id": "yfSCaPty0ROo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "\n",
        "class AF_CNNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 16, 5, padding=2)\n",
        "    self.conv2 = nn.Conv2d(16, 32, 5, padding=2)\n",
        "    self.conv3 = nn.Conv2d(32, 64, 5, padding=2)\n",
        "    self.conv4 = nn.Conv2d(64, 128, 5, padding=2)\n",
        "\n",
        "    self.dense1 = nn.Linear(1024, 1024)\n",
        "    self.dense2 = nn.Linear(1024, 256 )\n",
        "    self.dense3 = nn.Linear(256, 32)\n",
        "    self.dense4 = nn.Linear(32, 4)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = x.unsqueeze(1)\n",
        "      x = F.max_pool2d(F.logsigmoid(self.conv1(x)), kernel_size=2)\n",
        "      x = F.max_pool2d(F.logsigmoid(self.conv2(x)), kernel_size=2)\n",
        "      x = F.max_pool2d(F.logsigmoid(self.conv3(x)), kernel_size=2)\n",
        "      x = F.max_pool2d(F.logsigmoid(self.conv4(x)), kernel_size=2)\n",
        "      x = x.view(-1, 1024)\n",
        "      x = F.logsigmoid(self.dense1(x))\n",
        "      x = F.logsigmoid(self.dense2(x))\n",
        "      x = F.logsigmoid(self.dense3(x))\n",
        "      x = self.dense4(x)\n",
        "\n",
        "      return x \n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "net_af= AF_CNNet().to(device)"
      ],
      "metadata": {
        "id": "SZIOj4RgsaL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 2e-3\n",
        "optimizer = torch.optim.SGD(net_af.parameters(), lr = learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "num_epochs = 30\n",
        "\n",
        "print(f\"Device {device}\")\n",
        "\n",
        "train_loop_val(train_dataloader, net_af, loss_fn, optimizer, num_epochs)\n",
        "\n",
        "best_model = AF_CNNet().to(device)\n",
        "best_model.load_state_dict(torch.load('saved_model.pth'))\n",
        "best_model.eval()\n",
        "test_fn(test_dataloader, best_model ,loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNdalNLWnUDH",
        "outputId": "8b7ad9ca-1691-4143-b668-dfa241a1720b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device cpu\n",
            "Found better model in epoch 0! Saving... \n",
            "Found better model in epoch 1! Saving... \n",
            "Found better model in epoch 7! Saving... \n",
            "Found better model in epoch 19! Saving... \n",
            "Found better model in epoch 22! Saving... \n",
            "Found better model in epoch 25! Saving... \n",
            "Found better model in epoch 26! Saving... \n",
            "Found better model in epoch 29! Saving... \n",
            "Test Error: \n",
            " Accuracy: 61.5%, Avg loss: 0.058939, Avg f1: 0.018035 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "|  | ReLU|LogSigmoid |ELU| \n",
        "|----|-----|-----|----\n",
        "|Accuracy|67.8%|61.5%|73.4%|\n",
        "|f1| 0.021|0.018|0.046|"
      ],
      "metadata": {
        "id": "vY8bNMZm9uJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Step 4**: Learning rate scheduler"
      ],
      "metadata": {
        "id": "ZInG8wT8-Efa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_sch(dataloader, model, loss_fn, optimizer, Nepochs, scheduler):\n",
        "  size = len(dataloader.dataset)\n",
        "\n",
        "  max_val_f1 = 0.0\n",
        "\n",
        "  for epoch in range(Nepochs):\n",
        "\n",
        "      for batch, sample in enumerate(dataloader):\n",
        "        X = sample['X']\n",
        "        y = sample['label']\n",
        "\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X.float())\n",
        "        loss = loss_fn(pred, y) \n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "      \n",
        "\n",
        "      # Compute f1 score of validation set in this model\n",
        "      val_f1 = 0.0\n",
        "      for batch, sample in enumerate(val_dataloader):\n",
        "          X = sample['X']\n",
        "          y = sample['label']\n",
        "\n",
        "          X = X.to(device)\n",
        "          y = y.to(device)\n",
        "\n",
        "          # Compute prediction\n",
        "          pred = model(X.float())\n",
        "          val_f1 += f1_score(y.cpu(), pred.cpu().argmax(1), average = 'macro')\n",
        "\n",
        "      if val_f1 > max_val_f1:\n",
        "        max_val_f1 = val_f1\n",
        "        print(f\"Found better model in epoch {epoch}! Saving... \")\n",
        "        torch.save(model.state_dict(), 'saved_model.pth')\n",
        "\n",
        "      scheduler.step() \n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "DQuIUd-iddns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim.lr_scheduler\n",
        "\n",
        "learning_rate = 2e-3\n",
        "optimizer = torch.optim.SGD(net_af.parameters(), lr = learning_rate)\n",
        "scheduler =  torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9 ,verbose=True)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "num_epochs = 30\n",
        "\n",
        "print(f\"Device {device}\")\n",
        "\n",
        "train_sch(train_dataloader, net_af, loss_fn, optimizer, num_epochs, scheduler)\n",
        "\n",
        "best_model = AF_CNNet().to(device)\n",
        "best_model.load_state_dict(torch.load('saved_model.pth'))\n",
        "best_model.eval()\n",
        "test_fn(test_dataloader, best_model ,loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJZaA840Axqr",
        "outputId": "bde79cfb-37d7-4534-d57f-c4507f27a296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Device cpu\n",
            "Found better model in epoch 0! Saving... \n",
            "Adjusting learning rate of group 0 to 1.8000e-03.\n",
            "Adjusting learning rate of group 0 to 1.6200e-03.\n",
            "Found better model in epoch 2! Saving... \n",
            "Adjusting learning rate of group 0 to 1.4580e-03.\n",
            "Adjusting learning rate of group 0 to 1.3122e-03.\n",
            "Adjusting learning rate of group 0 to 1.1810e-03.\n",
            "Found better model in epoch 5! Saving... \n",
            "Adjusting learning rate of group 0 to 1.0629e-03.\n",
            "Adjusting learning rate of group 0 to 9.5659e-04.\n",
            "Adjusting learning rate of group 0 to 8.6093e-04.\n",
            "Adjusting learning rate of group 0 to 7.7484e-04.\n",
            "Adjusting learning rate of group 0 to 6.9736e-04.\n",
            "Found better model in epoch 10! Saving... \n",
            "Adjusting learning rate of group 0 to 6.2762e-04.\n",
            "Adjusting learning rate of group 0 to 5.6486e-04.\n",
            "Adjusting learning rate of group 0 to 5.0837e-04.\n",
            "Adjusting learning rate of group 0 to 4.5754e-04.\n",
            "Adjusting learning rate of group 0 to 4.1178e-04.\n",
            "Adjusting learning rate of group 0 to 3.7060e-04.\n",
            "Adjusting learning rate of group 0 to 3.3354e-04.\n",
            "Adjusting learning rate of group 0 to 3.0019e-04.\n",
            "Adjusting learning rate of group 0 to 2.7017e-04.\n",
            "Adjusting learning rate of group 0 to 2.4315e-04.\n",
            "Adjusting learning rate of group 0 to 2.1884e-04.\n",
            "Adjusting learning rate of group 0 to 1.9695e-04.\n",
            "Adjusting learning rate of group 0 to 1.7726e-04.\n",
            "Adjusting learning rate of group 0 to 1.5953e-04.\n",
            "Adjusting learning rate of group 0 to 1.4358e-04.\n",
            "Adjusting learning rate of group 0 to 1.2922e-04.\n",
            "Adjusting learning rate of group 0 to 1.1630e-04.\n",
            "Adjusting learning rate of group 0 to 1.0467e-04.\n",
            "Found better model in epoch 28! Saving... \n",
            "Adjusting learning rate of group 0 to 9.4203e-05.\n",
            "Adjusting learning rate of group 0 to 8.4782e-05.\n",
            "Test Error: \n",
            " Accuracy: 68.5%, Avg loss: 0.052504, Avg f1: 0.020544 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Step 5**: Batch Normalization\n"
      ],
      "metadata": {
        "id": "d9Xx7lCN-T2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BN_CNNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 16, 5, padding=2)\n",
        "    self.b1 = nn.BatchNorm2d(16)\n",
        "    self.conv2 = nn.Conv2d(16, 32, 5, padding=2)\n",
        "    self.b2 = nn.BatchNorm2d(32)\n",
        "    self.conv3 = nn.Conv2d(32, 64, 5, padding=2)\n",
        "    self.b3 = nn.BatchNorm2d(64)\n",
        "    self.conv4 = nn.Conv2d(64, 128, 5, padding=2)\n",
        "    self.b4 = nn.BatchNorm2d(128)\n",
        "\n",
        "    self.dense1 = nn.Linear(1024, 1024)\n",
        "    self.dense2 = nn.Linear(1024, 256)\n",
        "    self.dense3 = nn.Linear(256, 32)\n",
        "    self.dense4 = nn.Linear(32, 4)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = x.unsqueeze(1)\n",
        "      x = F.max_pool2d(F.relu(self.b1(self.conv1(x))), kernel_size=2)\n",
        "      x = F.max_pool2d(F.relu(self.b2(self.conv2(x))), kernel_size=2)\n",
        "      x = F.max_pool2d(F.relu(self.b3(self.conv3(x))), kernel_size=2)\n",
        "      x = F.max_pool2d(F.relu(self.b4(self.conv4(x))), kernel_size=2)\n",
        "      x = x.view(-1, 1024)\n",
        "\n",
        "      x = F.relu(self.dense1(x))\n",
        "      x = F.relu(self.dense2(x))\n",
        "      x = F.relu(self.dense3(x))\n",
        "      x = self.dense4(x)\n",
        "\n",
        "      return x \n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "net_bn= BN_CNNet().to(device)"
      ],
      "metadata": {
        "id": "nOa2lBN0hSuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sch(train_dataloader, net_bn, loss_fn, optimizer, num_epochs, scheduler)\n",
        "\n",
        "best_model = BN_CNNet().to(device)\n",
        "best_model.load_state_dict(torch.load('saved_model.pth'))\n",
        "best_model.eval()\n",
        "test_fn(test_dataloader, best_model ,loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yn9XR6mglAvj",
        "outputId": "5ef59a60-2786-4646-e437-fe587d57b690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found better model in epoch 0! Saving... \n",
            "Adjusting learning rate of group 0 to 7.6304e-05.\n",
            "Found better model in epoch 1! Saving... \n",
            "Adjusting learning rate of group 0 to 6.8674e-05.\n",
            "Adjusting learning rate of group 0 to 6.1806e-05.\n",
            "Found better model in epoch 3! Saving... \n",
            "Adjusting learning rate of group 0 to 5.5626e-05.\n",
            "Adjusting learning rate of group 0 to 5.0063e-05.\n",
            "Adjusting learning rate of group 0 to 4.5057e-05.\n",
            "Adjusting learning rate of group 0 to 4.0551e-05.\n",
            "Adjusting learning rate of group 0 to 3.6496e-05.\n",
            "Adjusting learning rate of group 0 to 3.2846e-05.\n",
            "Adjusting learning rate of group 0 to 2.9562e-05.\n",
            "Adjusting learning rate of group 0 to 2.6606e-05.\n",
            "Adjusting learning rate of group 0 to 2.3945e-05.\n",
            "Adjusting learning rate of group 0 to 2.1551e-05.\n",
            "Adjusting learning rate of group 0 to 1.9395e-05.\n",
            "Adjusting learning rate of group 0 to 1.7456e-05.\n",
            "Adjusting learning rate of group 0 to 1.5710e-05.\n",
            "Adjusting learning rate of group 0 to 1.4139e-05.\n",
            "Adjusting learning rate of group 0 to 1.2725e-05.\n",
            "Found better model in epoch 18! Saving... \n",
            "Adjusting learning rate of group 0 to 1.1453e-05.\n",
            "Adjusting learning rate of group 0 to 1.0308e-05.\n",
            "Adjusting learning rate of group 0 to 9.2768e-06.\n",
            "Adjusting learning rate of group 0 to 8.3491e-06.\n",
            "Adjusting learning rate of group 0 to 7.5142e-06.\n",
            "Adjusting learning rate of group 0 to 6.7628e-06.\n",
            "Adjusting learning rate of group 0 to 6.0865e-06.\n",
            "Adjusting learning rate of group 0 to 5.4779e-06.\n",
            "Adjusting learning rate of group 0 to 4.9301e-06.\n",
            "Adjusting learning rate of group 0 to 4.4371e-06.\n",
            "Adjusting learning rate of group 0 to 3.9934e-06.\n",
            "Adjusting learning rate of group 0 to 3.5940e-06.\n",
            "Test Error: \n",
            " Accuracy: 29.0%, Avg loss: 0.086320, Avg f1: 0.017794 \n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "29kPspHM-QMK"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}